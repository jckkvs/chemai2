"""frontend_streamlit/pages/evaluation_page.py - ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒšãƒ¼ã‚¸ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
from __future__ import annotations
import numpy as np
import pandas as pd
import streamlit as st


def render() -> None:
    st.markdown("## ğŸ“Š ãƒ¢ãƒ‡ãƒ«è©•ä¾¡")
    result = st.session_state.get("automl_result")
    if result is None:
        st.warning("âš ï¸ ã¾ãšAutoMLã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        if st.button("ğŸ¤– AutoMLã¸"):
            st.session_state["page"] = "automl"
            st.rerun()
        return

    df = st.session_state.get("df")
    target_col = st.session_state.get("target_col")

    # â”€â”€â”€ ã‚µãƒãƒªãƒ¼ã‚«ãƒ¼ãƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    col1, col2, col3, col4 = st.columns(4)
    metric_data = [
        (col1, result.best_model_key, "ğŸ† æœ€è‰¯ãƒ¢ãƒ‡ãƒ«"),
        (col2, result.task, "ğŸ“‹ ã‚¿ã‚¹ã‚¯"),
        (col3, f"{result.best_score:.4f}", f"ğŸ“ˆ {result.scoring}"),
        (col4, f"{result.elapsed_seconds:.1f}s", "â±ï¸ å­¦ç¿’æ™‚é–“"),
    ]
    for col, val, label in metric_data:
        with col:
            st.markdown(
                f'<div class="metric-card"><div class="metric-value" style="font-size:1.3rem">'
                f'{val}</div><div class="metric-label">{label}</div></div>',
                unsafe_allow_html=True
            )

    st.markdown("---")

    # â”€â”€â”€ ã‚¿ãƒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tab1, tab2, tab3, tab4 = st.tabs(
        ["ğŸ“ˆ äºˆæ¸¬ç²¾åº¦", "ğŸ“Š å…¨ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ", "ğŸ“‰ å­¦ç¿’æ›²ç·š", "ğŸ” æ®‹å·®/æ··åŒè¡Œåˆ—"]
    )

    # â”€â”€â”€ Tab1: äºˆæ¸¬ç²¾åº¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with tab1:
        if df is not None and target_col:
            X = df.drop(columns=[target_col])
            y_true = df[target_col].values
            try:
                y_pred = result.best_pipeline.predict(X)
                if result.task == "regression":
                    _show_regression_metrics(y_true, y_pred)
                else:
                    _show_classification_metrics(y_true, y_pred, result.best_pipeline, X)
            except Exception as e:
                st.error(f"äºˆæ¸¬ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            st.info("ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ç›®çš„å¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    # â”€â”€â”€ Tab2: å…¨ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with tab2:
        scores = result.all_scores if hasattr(result, "all_scores") else {}
        if scores:
            import plotly.express as px
            rows = []
            for key, cv_res in scores.items():
                mean_v = cv_res.get("mean_test_score")
                std_v = cv_res.get("std_test_score", 0)
                fit_t = cv_res.get("fit_time", np.array([0])).mean()
                rows.append({
                    "ãƒ¢ãƒ‡ãƒ«": key,
                    "CVå¹³å‡ã‚¹ã‚³ã‚¢": mean_v or 0,
                    "CVæ¨™æº–åå·®": std_v or 0,
                    "å­¦ç¿’æ™‚é–“(s)": round(fit_t, 2),
                    "æœ€è‰¯": "â­" if key == result.best_model_key else "",
                })
            cmp_df = pd.DataFrame(rows).sort_values("CVå¹³å‡ã‚¹ã‚³ã‚¢", ascending=False)
            st.dataframe(
                cmp_df.style.background_gradient(
                    subset=["CVå¹³å‡ã‚¹ã‚³ã‚¢"], cmap="RdYlGn"
                ),
                use_container_width=True,
            )

            # ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
            fig = px.bar(
                cmp_df, x="ãƒ¢ãƒ‡ãƒ«", y="CVå¹³å‡ã‚¹ã‚³ã‚¢",
                error_y="CVæ¨™æº–åå·®",
                color="CVå¹³å‡ã‚¹ã‚³ã‚¢",
                color_continuous_scale="RdYlGn",
                template="plotly_dark",
                title=f"å…¨ãƒ¢ãƒ‡ãƒ«CVæ¯”è¼ƒ ({result.scoring})",
                text=cmp_df["CVå¹³å‡ã‚¹ã‚³ã‚¢"].map("{:.3f}".format),
            )
            fig.update_layout(
                plot_bgcolor="rgba(0,0,0,0)",
                paper_bgcolor="rgba(0,0,0,0)",
                font=dict(color="#e0e0f0"),
                xaxis_tickangle=-30,
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("å…¨ãƒ¢ãƒ‡ãƒ«ã®CVã‚¹ã‚³ã‚¢ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ã¿è¡¨ç¤º
            st.markdown(f"**æœ€è‰¯ãƒ¢ãƒ‡ãƒ«**: `{result.best_model_key}` â€” ã‚¹ã‚³ã‚¢: `{result.best_score:.4f}`")

    # â”€â”€â”€ Tab3: å­¦ç¿’æ›²ç·š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with tab3:
        if df is not None and target_col:
            st.markdown("### ğŸ“‰ å­¦ç¿’æ›²ç·š")
            st.caption("ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å¢—ã‚„ã—ãŸã¨ãã®è¨“ç·´ãƒ»æ¤œè¨¼ã‚¹ã‚³ã‚¢ã®æ¨ç§»")
            cv_folds = st.slider("CVåˆ†å‰²æ•°", 2, 10, 5, key="lc_cv")
            scoring_lc = result.scoring if result.task == "regression" else "accuracy"

            if st.button("â–¶ï¸ å­¦ç¿’æ›²ç·šã‚’è¨ˆç®—", key="btn_lc"):
                with st.spinner("è¨ˆç®—ä¸­..."):
                    try:
                        from backend.data.benchmark import compute_learning_curve
                        X = df.drop(columns=[target_col])
                        y = df[target_col]
                        lc = compute_learning_curve(
                            result.best_pipeline, X, y,
                            scoring=scoring_lc, cv=cv_folds, n_points=8,
                        )
                        st.session_state["_lc_data"] = lc
                    except Exception as e:
                        st.error(f"âŒ å­¦ç¿’æ›²ç·šã®è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

            lc = st.session_state.get("_lc_data")
            if lc is not None:
                import plotly.graph_objects as go
                ts = lc["train_sizes"]
                fig = go.Figure()
                fig.add_trace(go.Scatter(
                    x=ts, y=lc["train_scores_mean"],
                    mode="lines+markers", name="è¨“ç·´ã‚¹ã‚³ã‚¢",
                    line=dict(color="#00d4ff"),
                    error_y=dict(array=lc["train_scores_std"],
                                 color="rgba(0,212,255,0.3)", thickness=1),
                ))
                fig.add_trace(go.Scatter(
                    x=ts, y=lc["val_scores_mean"],
                    mode="lines+markers", name="æ¤œè¨¼ã‚¹ã‚³ã‚¢",
                    line=dict(color="#ff6b9d"),
                    error_y=dict(array=lc["val_scores_std"],
                                 color="rgba(255,107,157,0.3)", thickness=1),
                ))
                fig.update_layout(
                    title=f"å­¦ç¿’æ›²ç·š ({scoring_lc})",
                    xaxis_title="ã‚µãƒ³ãƒ—ãƒ«æ•°", yaxis_title=scoring_lc,
                    plot_bgcolor="rgba(0,0,0,0)",
                    paper_bgcolor="rgba(0,0,0,0)",
                    font=dict(color="#e0e0f0"),
                    legend=dict(bgcolor="rgba(0,0,0,0)"),
                )
                st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    # â”€â”€â”€ Tab4: æ®‹å·®/æ··åŒè¡Œåˆ— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with tab4:
        if df is not None and target_col:
            X = df.drop(columns=[target_col])
            y_true = df[target_col].values
            try:
                y_pred = result.best_pipeline.predict(X)
                if result.task == "regression":
                    _show_residuals(y_true, y_pred)
                else:
                    _show_confusion_matrix(y_true, y_pred)
            except Exception as e:
                st.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            st.info("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")


# ============================================================
# å†…éƒ¨ãƒ˜ãƒ«ãƒ‘ãƒ¼
# ============================================================

def _show_regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> None:
    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
    import plotly.graph_objects as go

    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    mape = float(np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + 1e-8))) * 100)

    cols = st.columns(4)
    for col, val, label in [
        (cols[0], f"{rmse:.4f}", "RMSE"),
        (cols[1], f"{mae:.4f}", "MAE"),
        (cols[2], f"{r2:.4f}", "RÂ²"),
        (cols[3], f"{mape:.2f}%", "MAPE"),
    ]:
        with col:
            st.markdown(
                f'<div class="metric-card"><div class="metric-value">{val}</div>'
                f'<div class="metric-label">{label}</div></div>',
                unsafe_allow_html=True
            )

    st.markdown("---")
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=y_true, y=y_pred, mode="markers",
        marker=dict(color="#00d4ff", opacity=0.6, size=6), name="äºˆæ¸¬"
    ))
    rng = [float(min(y_true.min(), y_pred.min())), float(max(y_true.max(), y_pred.max()))]
    fig.add_trace(go.Scatter(
        x=rng, y=rng, mode="lines",
        line=dict(color="#fbbf24", dash="dash"), name="å®Œå…¨ä¸€è‡´"
    ))
    fig.update_layout(
        title="å®Ÿæ¸¬ vs äºˆæ¸¬", xaxis_title="å®Ÿæ¸¬å€¤", yaxis_title="äºˆæ¸¬å€¤",
        plot_bgcolor="rgba(0,0,0,0)", paper_bgcolor="rgba(0,0,0,0)",
        font=dict(color="#e0e0f0"), template="plotly_dark",
    )
    st.plotly_chart(fig, use_container_width=True)


def _show_classification_metrics(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    pipeline: Any = None,
    X: Any = None,
) -> None:
    from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_score, recall_score
    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average="weighted", zero_division=0)
    prec = precision_score(y_true, y_pred, average="weighted", zero_division=0)
    rec = recall_score(y_true, y_pred, average="weighted", zero_division=0)

    cols = st.columns(4)
    for col, val, label in [
        (cols[0], f"{acc:.4f}", "Accuracy"),
        (cols[1], f"{f1:.4f}", "F1 (weighted)"),
        (cols[2], f"{prec:.4f}", "Precision"),
        (cols[3], f"{rec:.4f}", "Recall"),
    ]:
        with col:
            st.markdown(
                f'<div class="metric-card"><div class="metric-value">{val}</div>'
                f'<div class="metric-label">{label}</div></div>',
                unsafe_allow_html=True
            )
    st.markdown("---")
    st.text(classification_report(y_true, y_pred, zero_division=0))


def _show_residuals(y_true: np.ndarray, y_pred: np.ndarray) -> None:
    """æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆã€‚"""
    import plotly.express as px
    residuals = y_pred - y_true
    res_df = pd.DataFrame({"äºˆæ¸¬å€¤": y_pred, "æ®‹å·®": residuals})

    col1, col2 = st.columns(2)
    with col1:
        fig1 = px.scatter(
            res_df, x="äºˆæ¸¬å€¤", y="æ®‹å·®",
            title="æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ (äºˆæ¸¬å€¤ vs æ®‹å·®)",
            template="plotly_dark",
            color_discrete_sequence=["#00d4ff"],
            opacity=0.6,
        )
        fig1.add_hline(y=0, line_dash="dash", line_color="#fbbf24")
        fig1.update_layout(
            plot_bgcolor="rgba(0,0,0,0)", paper_bgcolor="rgba(0,0,0,0)",
            font=dict(color="#e0e0f0"),
        )
        st.plotly_chart(fig1, use_container_width=True)
    with col2:
        fig2 = px.histogram(
            res_df, x="æ®‹å·®", nbins=30,
            title="æ®‹å·®ã®åˆ†å¸ƒ",
            template="plotly_dark",
            color_discrete_sequence=["#7b2ff7"],
        )
        fig2.update_layout(
            plot_bgcolor="rgba(0,0,0,0)", paper_bgcolor="rgba(0,0,0,0)",
            font=dict(color="#e0e0f0"),
        )
        st.plotly_chart(fig2, use_container_width=True)


def _show_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray) -> None:
    """æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ–ã€‚"""
    from sklearn.metrics import confusion_matrix
    import plotly.figure_factory as ff

    labels = np.unique(np.concatenate([y_true, y_pred]))
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    labels_str = [str(l) for l in labels]

    fig = ff.create_annotated_heatmap(
        z=cm[::-1],
        x=labels_str,
        y=labels_str[::-1],
        colorscale="Blues",
        showscale=True,
    )
    fig.update_layout(
        title="æ··åŒè¡Œåˆ—",
        xaxis_title="äºˆæ¸¬ã‚¯ãƒ©ã‚¹",
        yaxis_title="å®Ÿæ¸¬ã‚¯ãƒ©ã‚¹",
        plot_bgcolor="rgba(0,0,0,0)",
        paper_bgcolor="rgba(0,0,0,0)",
        font=dict(color="#e0e0f0"),
    )
    st.plotly_chart(fig, use_container_width=True)


# type alias for type checking only
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from typing import Any
